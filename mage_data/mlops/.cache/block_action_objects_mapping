{"block_file": {"custom/dynamic_model_block.py:custom:python:dynamic model block": {"content": "from typing import Dict, List, Tuple\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef models(*args, **kwargs) -> Tuple[List[str], List[Dict[str, str]]]:\n    \"\"\"\n    models: comma separated strings\n        linear_model.Lasso\n        linear_model.LinearRegression\n        svm.LinearSVR\n        ensemble.ExtraTreesRegressor\n        ensemble.GradientBoostingRegressor\n        ensemble.RandomForestRegressor\n    \"\"\"\n    model_names: str = kwargs.get(\n        'models', 'linear_model.LinearRegression,linear_model.Lasso'\n    )\n    child_data: List[str] = [\n        model_name.strip() for model_name in model_names.split(',')\n    ]\n    child_metadata: List[Dict] = [\n        dict(block_uuid=model_name.split('.')[-1]) for model_name in child_data\n    ]\n\n    return child_data, child_metadata", "file_path": "custom/dynamic_model_block.py", "language": "python", "type": "custom", "uuid": "dynamic_model_block"}, "custom/dynamic_model_block.sql:custom:sql:dynamic model block": {"content": "-- Docs: https://docs.mage.ai/guides/sql-blocks\n", "file_path": "custom/dynamic_model_block.sql", "language": "sql", "type": "custom", "uuid": "dynamic_model_block"}, "custom/magnificent_glitter.py:custom:python:magnificent glitter": {"content": "import requests\nfrom io import BytesIO\nfrom typing import List\n\nimport pandas as pd\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef ingest_files(**kwargs) -> pd.DataFrame:\n    dfs: List[pd.DataFrame] = []\n\n    for year, months in [(2024, (1, 3))]:\n        for i in range(*months):\n            response = requests.get(\n                'https://github.com/mage-ai/datasets/raw/master/taxi/green'\n                f'/{year}/{i:02d}.parquet'\n            )\n\n            if response.status_code != 200:\n                raise Exception(response.text)\n\n            df = pd.read_parquet(BytesIO(response.content))\n            dfs.append(df)\n\n    return pd.concat(dfs)", "file_path": "custom/magnificent_glitter.py", "language": "python", "type": "custom", "uuid": "magnificent_glitter"}, "data_exporters/build.py:data_exporter:python:build": {"content": "from typing import List, Tuple\n\nfrom pandas import DataFrame, Series\nfrom scipy.sparse._csr import csr_matrix\nfrom sklearn.base import BaseEstimator\n\nfrom mlops.utils.data_preparation.encoders import vectorize_features\nfrom mlops.utils.data_preparation.feature_selector import select_features\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_exporter\ndef export(\n    data: Tuple[DataFrame, DataFrame, DataFrame], *args, **kwargs\n) -> Tuple[\n    csr_matrix,\n    csr_matrix,\n    csr_matrix,\n    Series,\n    Series,\n    Series,\n    BaseEstimator,\n]:\n    df, df_train, df_val = data\n    target = kwargs.get('target', 'duration')\n\n    X, _, _ = vectorize_features(select_features(df))\n    y: Series = df[target]\n\n    X_train, X_val, dv = vectorize_features(\n        select_features(df_train),\n        select_features(df_val),\n    )\n    y_train = df_train[target]\n    y_val = df_val[target]\n\n    return X, X_train, X_val, y, y_train, y_val, dv\n\n\n@test\ndef test_dataset(\n    X: csr_matrix,\n    X_train: csr_matrix,\n    X_val: csr_matrix,\n    y: Series,\n    y_train: Series,\n    y_val: Series,\n    *args,\n) -> None:\n    assert (\n        X.shape[0] == 105870\n    ), f'Entire dataset should have 105870 examples, but has {X.shape[0]}'\n    assert (\n        X.shape[1] == 7027\n    ), f'Entire dataset should have 7027 features, but has {X.shape[1]}'\n    assert (\n        len(y.index) == X.shape[0]\n    ), f'Entire dataset should have {X.shape[0]} examples, but has {len(y.index)}'\n\n\n@test\ndef test_training_set(\n    X: csr_matrix,\n    X_train: csr_matrix,\n    X_val: csr_matrix,\n    y: Series,\n    y_train: Series,\n    y_val: Series,\n    *args,\n) -> None:\n    assert (\n        X_train.shape[0] == 54378\n    ), f'Training set for training model should have 54378 examples, but has {X_train.shape[0]}'\n    assert (\n        X_train.shape[1] == 5094\n    ), f'Training set for training model should have 5094 features, but has {X_train.shape[1]}'\n    assert (\n        len(y_train.index) == X_train.shape[0]\n    ), f'Training set for training model should have {X_train.shape[0]} examples, but has {len(y_train.index)}'\n\n\n@test\ndef test_validation_set(\n    X: csr_matrix,\n    X_train: csr_matrix,\n    X_val: csr_matrix,\n    y: Series,\n    y_train: Series,\n    y_val: Series,\n    *args,\n) -> None:\n    assert (\n        X_val.shape[0] == 51492\n    ), f'Training set for validation should have 51492 examples, but has {X_val.shape[0]}'\n    assert (\n        X_val.shape[1] == 5094\n    ), f'Training set for validation should have 5094 features, but has {X_val.shape[1]}'\n    assert (\n        len(y_val.index) == X_val.shape[0]\n    ), f'Training set for training model should have {X_val.shape[0]} examples, but has {len(y_val.index)}'", "file_path": "data_exporters/build.py", "language": "python", "type": "data_exporter", "uuid": "build"}, "data_exporters/train_model.py:data_exporter:python:train model": {"content": "\nfrom typing import Dict, Tuple, Union\n\nfrom pandas import Series\nfrom scipy.sparse._csr import csr_matrix\nfrom xgboost import Booster\n\nfrom mlops.utils.models.xgboost import build_data, fit_model\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef train(\n    \n    settings: Tuple[\n        Dict[str, Union[bool, float, int, str]],\n        csr_matrix,\n        Series,\n    ],\n    training_set: Dict[str, Union[Series, csr_matrix]],\n    **kwargs\n) -> Tuple[Booster, csr_matrix, Series]:\n    hyperparameters, X, y = settings\n\n    # Test training a model with low max depth\n    # so that the output renders a reasonably sized plot tree.\n    if kwargs.get('max_depth'):\n        hyperparameters['max_depth'] = int(kwargs.get('max_depth'))\n\n    model = fit_model(\n        build_data(X, y),\n        hyperparameters,\n        verbose_eval=kwargs.get('verbose_eval', 100),\n    )\n\n    # DictVectorizer to transform features for online inference.\n    vectorizer = training_set['build'][6]\n    return model, vectorizer\n\n", "file_path": "data_exporters/train_model.py", "language": "python", "type": "data_exporter", "uuid": "train_model"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/build_model.py:data_exporter:python:build model": {"content": "from typing import Callable, Dict, Tuple, Union\n\nfrom pandas import Series\nfrom scipy.sparse._csr import csr_matrix\nfrom sklearn.base import BaseEstimator\n\nfrom mlops.utils.models.sklearn import load_class, train_model\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef train(\n    settings: Tuple[\n        Dict[str, Union[bool, float, int, str]],\n        csr_matrix,\n        Series,\n        Dict[str, Union[Callable[..., BaseEstimator], str]],\n    ],\n    **kwargs,\n) -> Tuple[BaseEstimator, Dict[str, str]]:\n    hyperparameters, X, y, model_info = settings\n\n    model_class = model_info['cls']\n    model = model_class(**hyperparameters)\n    model.fit(X, y)\n\n    return model, model_info", "file_path": "data_exporters/build_model.py", "language": "python", "type": "data_exporter", "uuid": "build_model"}, "data_loaders/ingest.py:data_loader:python:ingest": {"content": "import requests\nfrom io import BytesIO\nfrom typing import List\n\nimport pandas as pd\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\n@data_loader\ndef ingest_files(**kwargs) -> pd.DataFrame:\n    dfs: List[pd.DataFrame] = []\n\n    for year, months in [(2024, (1, 3))]:\n        for i in range(*months):\n            response = requests.get(\n                'https://github.com/mage-ai/datasets/raw/master/taxi/green'\n                f'/{year}/{i:02d}.parquet'\n            )\n\n            if response.status_code != 200:\n                raise Exception(response.text)\n\n            df = pd.read_parquet(BytesIO(response.content))\n            dfs.append(df)\n\n    return pd.concat(dfs)", "file_path": "data_loaders/ingest.py", "language": "python", "type": "data_loader", "uuid": "ingest"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/hyper_parameter_tuning.py:transformer:python:hyper parameter tuning": {"content": "from typing import Callable, Dict, Tuple, Union\n\nfrom pandas import Series\nfrom scipy.sparse._csr import csr_matrix\nfrom sklearn.base import BaseEstimator\n\nfrom mlops.utils.models.sklearn import load_class, tune_hyperparameters\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef hyperparameter_tuning(\n    model_class_name: str,\n    training_set: Dict[str, Union[Series, csr_matrix]],\n   \n    *args,\n    **kwargs,\n) -> Tuple[\n    Dict[str, Union[bool, float, int, str]],\n    csr_matrix,\n    Series,\n    Callable[..., BaseEstimator],\n]:\n    \n    X, X_train, X_val, y, y_train, y_val, _ = training_set['build']\n\n    model_class = load_class(model_class_name)\n\n    hyperparameters = tune_hyperparameters(\n        model_class,\n        X_train=X_train,\n        y_train=y_train,\n        X_val=X_val,\n        y_val=y_val,\n        max_evaluations=kwargs.get('max_evaluations', 1),\n        random_state=kwargs.get('random_state', 3),\n    )\n\n    return hyperparameters, X, y, dict(cls=model_class, name=model_class_name)", "file_path": "transformers/hyper_parameter_tuning.py", "language": "python", "type": "transformer", "uuid": "hyper_parameter_tuning"}, "transformers/prepare.py:transformer:python:prepare": {"content": "from typing import Tuple\n\nimport pandas as pd\n\nfrom mlops.utils.data_preparation.cleaning import clean\nfrom mlops.utils.data_preparation.feature_engineering import combine_features\nfrom mlops.utils.data_preparation.feature_selector import select_features\nfrom mlops.utils.data_preparation.splitters import split_on_value\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef transform(\n    df: pd.DataFrame, **kwargs\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    split_on_feature = kwargs.get('split_on_feature')\n    split_on_feature_value = kwargs.get('split_on_feature_value')\n    target = kwargs.get('target')\n\n    df = clean(df)\n    df = combine_features(df)\n    df = select_features(df, features=[split_on_feature, target])\n\n    df_train, df_val = split_on_value(\n        df,\n        split_on_feature,\n        split_on_feature_value,\n    )\n\n    return df, df_train, df_val", "file_path": "transformers/prepare.py", "language": "python", "type": "transformer", "uuid": "prepare"}, "transformers/hyperparameter_tuning.py:transformer:python:hyperparameter tuning": {"content": "\nfrom typing import Dict, Tuple, Union\n\nimport numpy as np\nimport xgboost as xgb\nfrom pandas import Series\nfrom scipy.sparse._csr import csr_matrix\n\nfrom mlops.utils.logging import track_experiment\nfrom mlops.utils.models.xgboost import build_data, tune_hyperparameters\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef hyperparameter_tuning(\n    training_set: Dict[str, Union[Series, csr_matrix]],\n    **kwargs,\n) -> Tuple[\n    Dict[str, Union[bool, float, int, str]],\n    csr_matrix,\n    Series,\n]:\n    X, X_train, X_val, y, y_train, y_val, _ = training_set['build']\n\n    training = build_data(X_train, y_train)\n    validation = build_data(X_val, y_val)\n\n    print(\"Kwargs in the tranformer\", kwargs)\n    best_hyperparameters = tune_hyperparameters(\n        training,\n        validation,\n        callback=lambda **opts: track_experiment(**{**opts, **kwargs}),\n        **kwargs,\n    )\n\n    return best_hyperparameters, X_train, y_train\n", "file_path": "transformers/hyperparameter_tuning.py", "language": "python", "type": "transformer", "uuid": "hyperparameter_tuning"}, "pipelines/example_pipeline/__init__.py:pipeline:python:example pipeline/  init  ": {"content": "", "file_path": "pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "example_pipeline/__init__"}, "pipelines/example_pipeline/metadata.yaml:pipeline:yaml:example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "example_pipeline/metadata"}, "pipelines/train_model/__init__.py:pipeline:python:train model/  init  ": {"content": "", "file_path": "pipelines/train_model/__init__.py", "language": "python", "type": "pipeline", "uuid": "train_model/__init__"}, "pipelines/train_model/metadata.yaml:pipeline:yaml:train model/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    global_data_product:\n      uuid: training_set\n  downstream_blocks:\n  - hyper_parameter_tuning\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: training_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: global_data_product\n  upstream_blocks: []\n  uuid: training_data\n- all_upstream_blocks_executed: true\n  color: blue\n  configuration:\n    dynamic: true\n  downstream_blocks:\n  - hyper_parameter_tuning\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: dynamic_model_block\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks: []\n  uuid: dynamic_model_block\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - build_model\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: hyper_parameter_tuning\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - dynamic_model_block\n  - training_data\n  uuid: hyper_parameter_tuning\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: build_model\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - hyper_parameter_tuning\n  uuid: build_model\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-12-04 14:03:39.130877+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: train_model\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: train_model\nvariables_dir: /home/src/mage_data/mlops\nwidgets: []\n", "file_path": "pipelines/train_model/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "train_model/metadata"}, "pipelines/train_xgboost/__init__.py:pipeline:python:train xgboost/  init  ": {"content": "", "file_path": "pipelines/train_xgboost/__init__.py", "language": "python", "type": "pipeline", "uuid": "train_xgboost/__init__"}, "pipelines/train_xgboost/metadata.yaml:pipeline:yaml:train xgboost/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    global_data_product:\n      uuid: training_set\n  downstream_blocks:\n  - hyperparameter_tuning\n  - train_model\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: training-data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: global_data_product\n  upstream_blocks: []\n  uuid: training_data\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - train_model\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: hyperparameter-tuning\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - training_data\n  uuid: hyperparameter_tuning\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: train_model\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - hyperparameter_tuning\n  - training_data\n  uuid: train_model\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-12-13 06:03:29.974191+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: train-xgboost\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: train_xgboost\nvariables:\n  early_stopping_rounds: 1\n  max_depth: 6\n  max_evaluations: 1\nvariables_dir: /home/src/mage_data/mlops\nwidgets: []\n", "file_path": "pipelines/train_xgboost/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "train_xgboost/metadata"}, "pipelines/luminous_labyrinth/__init__.py:pipeline:python:luminous labyrinth/  init  ": {"content": "", "file_path": "pipelines/luminous_labyrinth/__init__.py", "language": "python", "type": "pipeline", "uuid": "luminous_labyrinth/__init__"}, "pipelines/luminous_labyrinth/metadata.yaml:pipeline:yaml:luminous labyrinth/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: data_loaders/ingest.py\n  downstream_blocks:\n  - prepare\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - build\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: prepare\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - ingest\n  uuid: prepare\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: build\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - prepare\n  uuid: build\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-12-04 05:42:34.340006+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: luminous labyrinth\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: luminous_labyrinth\nvariables:\n  split_on_feature: lpep_pickup_datetime\n  split_on_feature_value: '2024-02-01'\n  target: duration\nvariables_dir: /home/src/mage_data/mlops\nwidgets: []\n", "file_path": "pipelines/luminous_labyrinth/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "luminous_labyrinth/metadata"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}